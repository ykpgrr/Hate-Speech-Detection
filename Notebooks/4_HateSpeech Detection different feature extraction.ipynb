{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "import re\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "from textstat.textstat import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @__Junebugg: @VoiceOfDStreetz hell yea save...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>How the hell was David Murphy's hit not a home...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @FeeelGreatness: You don't know where your ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Thats some hoe shit doe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>I just want vanilla Oreos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19821</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @davegetnmoney: I beat the pussy up up up u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19822</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @RT_America: Russell Brand mocks Bill O'Rei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19823</th>\n",
       "      <td>1</td>\n",
       "      <td>@_B_R_Y_C_E_ what happened to going fishing bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19824</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @BriannDominguez: Gasoline - daddy Yankee &amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19825</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @LowkeyStoner_: Gay girls really do get mor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19826 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       class                                              tweet\n",
       "0          1  RT @__Junebugg: @VoiceOfDStreetz hell yea save...\n",
       "1          2  How the hell was David Murphy's hit not a home...\n",
       "2          1  RT @FeeelGreatness: You don't know where your ...\n",
       "3          1                            Thats some hoe shit doe\n",
       "4          2                          I just want vanilla Oreos\n",
       "...      ...                                                ...\n",
       "19821      1  RT @davegetnmoney: I beat the pussy up up up u...\n",
       "19822      2  RT @RT_America: Russell Brand mocks Bill O'Rei...\n",
       "19823      1  @_B_R_Y_C_E_ what happened to going fishing bi...\n",
       "19824      2  RT @BriannDominguez: Gasoline - daddy Yankee &...\n",
       "19825      1  RT @LowkeyStoner_: Gay girls really do get mor...\n",
       "\n",
       "[19826 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/hate_speech_data_train.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['class', 'tweet'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_pickle_file='pickle/mask_pickle'\n",
    "if os.path.exists(mask_pickle_file):\n",
    "    file = open(mask_pickle_file,'rb')\n",
    "    msk = pickle.load(file, encoding='latin1')\n",
    "else:\n",
    "    msk = np.random.rand(len(df)) < 0.8\n",
    "    outfile = open(mask_pickle_file,'wb')\n",
    "    pickle.dump(msk,outfile)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15853\n",
      "3973\n"
     ]
    }
   ],
   "source": [
    "train = df[msk]\n",
    "test = df[~msk]\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12315\n",
       "2     2618\n",
       "0      920\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3063\n",
       "2     678\n",
       "0     232\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns key:\n",
    "count = number of CrowdFlower users who coded each tweet (min is 3, sometimes more users coded a tweet when judgments were determined to be unreliable by CF).\n",
    "\n",
    "\n",
    "hate_speech = number of CF users who judged the tweet to be hate speech.\n",
    "\n",
    "\n",
    "offensive_language = number of CF users who judged the tweet to be offensive.\n",
    "\n",
    "\n",
    "neither = number of CF users who judged the tweet to be neither offensive nor non-offensive.\n",
    "\n",
    "\n",
    "class = class label for majority of CF users.\n",
    "\n",
    "    0 - hate speech\n",
    "    1 - offensive  language\n",
    "    2 - neither\n",
    "\n",
    "tweet = raw tweet text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11ed8aa90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['class'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This histogram shows the imbalanced nature of the task - most tweets containing \"hate\" words as defined by Hatebase were \n",
    "only considered to be offensive by the CF coders. More tweets were considered to be neither hate speech nor offensive language than were considered hate speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=df.tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "#tokenize each tweet into a list; just keep useful words\n",
    "#return a list in which each element denotes the list of a tweet\n",
    "def cleanData(dataSet):\n",
    "    '''tokenize each tweet into a list; just keep useful words;\n",
    "    return a list in which each element denotes the list of each tweet'''\n",
    "    tweets=dataSet\n",
    "    #build tables to remove punctuations\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    #add and delete words from stop_words\n",
    "    stop_words = stopwords.words('english')[:]\n",
    "    stop_words.extend([\"#ff\", \"ff\", \"rt\"])\n",
    "    stop_words.remove('not')\n",
    "    stop_words.remove('no')\n",
    "    #build lemmatizer\n",
    "    lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "    #build dictionary to convert contractions to standard lexicons\n",
    "    contractions = { \n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"cause\": \"because\", \n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he had\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he shall\",\n",
    "    \"he'll've\": \"he shall have\",\n",
    "    \"he's\": \"he has\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how has\",\n",
    "    \"i'd\": \"I had\",\n",
    "    \"i'd've\": \"I would have\",\n",
    "    \"i'll\": \"I shall\",\n",
    "    \"i'll've\": \"I shall have\",\n",
    "    \"i'm\": \"I am\",\n",
    "    \"i've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it had\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it shall\",\n",
    "    \"it'll've\": \"it shall have\",\n",
    "    \"it's\": \"it has\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she had\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she shall\",\n",
    "    \"she'll've\": \"she shall have\",\n",
    "    \"she's\": \"she has\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that has\",\n",
    "    \"there'd\": \"there had\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there has\",\n",
    "    \"they'd\": \"they had\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they shall\",\n",
    "    \"they'll've\": \"they shall have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we had\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what shall\",\n",
    "    \"what'll've\": \"what shall have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what has\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when has\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where has\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who shall\",\n",
    "    \"who'll've\": \"who shall have\",\n",
    "    \"who's\": \"who has\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why has\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you had\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you shall\",\n",
    "    \"you'll've\": \"you shall have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"ya\": \"you\",\n",
    "    \"im\": \"I am\"}\n",
    "    #dont; cant; cannot;\n",
    "    \n",
    "    cleandata = []\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        #get rid of urls\n",
    "        giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|''[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        tweet = re.sub(giant_url_regex, '', tweet)\n",
    "        snon1 = '&#[0-9]*;'\n",
    "        tweet = re.sub(snon1, ' ', tweet)\n",
    "        snon2 = '&#[0-9]*'\n",
    "        tweet = re.sub(snon2, ' ', tweet)\n",
    "        \n",
    "        tokens = tweet.split()\n",
    "        cleanstring = []\n",
    "        for o in tokens:\n",
    "            # change words to lowercase\n",
    "            o = o.lower()\n",
    "            # clean words that start as @\n",
    "            if o.startswith('@'): continue\n",
    "            if o.startswith(';@'): continue\n",
    "            if o.startswith('\"@'): continue\n",
    "            #expand contractions\n",
    "            o = contractions.get(o,o)\n",
    "            # remove punctuation from each token\n",
    "            o = o.translate(table)  \n",
    "            # filter out short tokens \n",
    "            if len(o) < 2: continue\n",
    "            # remove remaining tokens that are not alphabetic\n",
    "            if not o.isalpha(): continue\n",
    "            # filter out stop words\n",
    "            if o in stop_words: continue\n",
    "            # Stem and lemmatize the words\n",
    "            o = lemma.lemmatize(o)\n",
    "            cleanstring.append(o)\n",
    "        cleandata.append(cleanstring)    \n",
    "    return cleandata\n",
    "\n",
    "def basicclean(text):\n",
    "    '''for a single tweet, remove multi-space, URL, @. \n",
    "    return a string'''\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|''[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    text = re.sub(giant_url_regex, ' ', text)\n",
    "    text = re.sub(mention_regex, ' ', text)\n",
    "    text = re.sub(space_pattern, ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data, clean and return line of tokens\n",
    "def dataFilter_train(dataSet):\n",
    "    '''clean train data\n",
    "    return a list, each element denote the string of each tweet'''\n",
    "    tokens = cleanData(dataSet)\n",
    "    lines = []\n",
    "    # filter by vocab\n",
    "    for element in tokens:\n",
    "        tk = ' '.join(element)\n",
    "        lines.append(tk)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data, clean and return line of tokens\n",
    "def dataFilter(dataSet, vocab):\n",
    "    '''clean test data; filter tokens according to vocabulary build in train; \n",
    "    return a list, each element denote the string of each tweet'''\n",
    "    tokens = cleanData(dataSet)\n",
    "    lines =[]\n",
    "    # filter by vocab\n",
    "    for element in tokens:\n",
    "        tmp = [w for w in element if w in vocab]\n",
    "        tk = ' '.join(tmp)\n",
    "        lines.append(tk)\n",
    "    return lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n-gram features for texts\n",
    "def f_ngram(train, test, mode='tfidf', binary=1, ngram=(1,1), min_c=1):\n",
    "    '''exact n-gram feacturs\n",
    "    return: feature array (fgram); feature vocabulary (vocab)\n",
    "    input: raw train and test data; ngram = (n,n) denote n_gram and ngram=(1,2) denote\n",
    "    1_gram and 2_gram; tokens with count below min_c are cut off. \n",
    "    '''\n",
    "    if mode == 'tfidf':\n",
    "        if binary==1:\n",
    "            gram = text.TfidfVectorizer(ngram_range=ngram, binary=True, min_df=min_c)\n",
    "        else:\n",
    "            gram = text.TfidfVectorizer(ngram_range=ngram, min_df=min_c)\n",
    "        \n",
    "    else: #mode=count\n",
    "        if binary==1:\n",
    "            gram = text.CountVectorizer(ngram_range=ngram, binary=True, min_df=min_c)\n",
    "        else:\n",
    "            gram = text.CountVectorizer(ngram_range=ngram, min_df=min_c)\n",
    "    train = dataFilter_train(train)\n",
    "    gram = gram.fit(train)\n",
    "    vocab = gram.get_feature_names()\n",
    "    fgram_train = gram.transform(train).toarray()\n",
    "    test = dataFilter(test, vocab)\n",
    "    fgram_test = gram.transform(test).toarray()\n",
    "    return (fgram_train, fgram_test, vocab)\n",
    "\n",
    "#property of words in tweets\n",
    "def data_Pos(data):\n",
    "    '''get property of words.\n",
    "    return a list in which each element denotes the list of each tweet\n",
    "    '''\n",
    "    dtags = []\n",
    "    for element in data:\n",
    "        element = element.split()\n",
    "        tag1 = nltk.pos_tag(element)\n",
    "        tag = [x[1] for x in tag1]\n",
    "        tagstr = \" \".join(tag)\n",
    "        dtags.append(tagstr)\n",
    "    return dtags\n",
    "\n",
    "def f_ngram_pos(train, test, mode='tfidf', binary=1, ngram=(1,1), min_c=1):\n",
    "    '''exact n-gram feacturs based on word property information\n",
    "    return: feature array (fgram); feature vocabulary (vocab)\n",
    "    input: raw train and test data; ngram = (n,n), denote n_gram; \n",
    "           tokens with count below min_c are cut off. \n",
    "    '''\n",
    "    if mode == 'tfidf':\n",
    "        if binary==1:\n",
    "            gram = text.TfidfVectorizer(ngram_range=ngram, binary=True, min_df=min_c)\n",
    "        else:\n",
    "            gram = text.TfidfVectorizer(ngram_range=ngram, min_df=min_c)  \n",
    "    else: #mode=count\n",
    "        if binary==1:\n",
    "            gram = text.CountVectorizer(ngram_range=ngram, binary=True, min_df=min_c)\n",
    "        else:\n",
    "            gram = text.CountVectorizer(ngram_range=ngram, min_df=min_c)\n",
    "    train = dataFilter_train(train)\n",
    "    train_pos = data_Pos(train)\n",
    "    gram.fit(train_pos)\n",
    "    vocab = gram.get_feature_names()\n",
    "    fgram_train = gram.transform(train_pos).toarray()\n",
    "    test = dataFilter_train(test)\n",
    "    test_pos = data_Pos(test)\n",
    "    fgram_test = gram.transform(test_pos).toarray()\n",
    "    return (fgram_train, fgram_test, vocab)\n",
    "\n",
    "def count_twitter_objs(text):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "    4) hashtags with HASHTAGHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned.\n",
    "    \n",
    "    Returns counts of urls, mentions, and hashtags.\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|''[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    text = re.sub(space_pattern, ' ', text)\n",
    "    text = re.sub(giant_url_regex, 'URLHERE', text)\n",
    "    text = re.sub(mention_regex, 'MENTIONHERE', text)\n",
    "    text = re.sub(hashtag_regex, 'HASHTAGHERE', text)\n",
    "    return(text.count('URLHERE'), text.count('MENTIONHERE'), text.count('HASHTAGHERE'))\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as sia\n",
    "sentiment_analyzer = sia()\n",
    "def other_features(tweet):\n",
    "    ''' return a list of features, including Sentiment scores, Text and Readability scores.\n",
    "    input: a single tweet text\n",
    "    '''\n",
    "    #Sentiment scores: positive or negative\n",
    "    sentiment = sentiment_analyzer.polarity_scores(tweet)\n",
    "    #text statistics\n",
    "    words = basicclean(tweet) #Get text only\n",
    "    syllables = textstat.syllable_count(words) #count syllables in words\n",
    "    num_chars = len(words) #num chars in words\n",
    "    num_chars_total = len(tweet)\n",
    "    num_terms = len(tweet.split())\n",
    "    num_words = len(words.split())\n",
    "    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_words = len(set(words.split()))\n",
    "    \n",
    "    #Flesch-Kincaid grade level: measure readability of text\n",
    "    FKg = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59,1)\n",
    "    #Flesch readability ease: measure readability of text\n",
    "    Fre = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)),2)\n",
    "    \n",
    "    #Count #, @, and http://\n",
    "    twitter_objs = count_twitter_objs(tweet) \n",
    "    retweet = 0 #是否转发？\n",
    "    if \"RT \" in words:\n",
    "        retweet = 1\n",
    "    features = [FKg, Fre, syllables, avg_syl, num_chars, num_chars_total, num_terms, \n",
    "                num_words, num_unique_words, sentiment['neg'], sentiment['pos'], sentiment['neu'], \n",
    "                sentiment['compound'], twitter_objs[2], twitter_objs[1], twitter_objs[0], retweet]\n",
    "    return features\n",
    "\n",
    "def f_others(rowdata):\n",
    "    '''get other features\n",
    "    return feature matrix\n",
    "    input: row data\n",
    "    '''\n",
    "    feats=[]\n",
    "    for element in rowdata:\n",
    "        feats.append(other_features(element))\n",
    "    return np.array(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw features bases on functions\n",
    "def features(train, test, ngram=(1,2), b=1, mc=5, ngram_p=None, b_p=1, mc_p=2, other_f=1, mode='tfidf'):\n",
    "    '''return a tuple including features matrices for train and test \n",
    "    '''\n",
    "    features_train, features_test, vocab1 = f_ngram(train, test, mode=mode, binary=b, ngram=(ngram[0],ngram[0]), min_c=mc)\n",
    "    if len(ngram)>1:\n",
    "        for i in range(len(ngram)-1):\n",
    "            f_train, f_test, vo = f_ngram(train, test, mode=mode, binary=b, ngram=(ngram[i+1],ngram[i+1]), min_c=mc)\n",
    "            features_train = np.concatenate((features_train, f_train), axis=1)\n",
    "            features_test = np.concatenate((features_test, f_test), axis=1)\n",
    "    if ngram_p != None:\n",
    "        for i in range(len(ngram_p)):\n",
    "            f_train, f_test, vo = f_ngram_pos(train, test, mode=mode, binary=b_p, ngram=(ngram_p[i],ngram_p[i]), min_c=mc_p)\n",
    "            features_train = np.concatenate((features_train, f_train), axis=1)\n",
    "            features_test = np.concatenate((features_test, f_test), axis=1)\n",
    "    if other_f == 1:\n",
    "        f_train = f_others(train)\n",
    "        f_test = f_others(test)\n",
    "        features_train = np.concatenate((features_train, f_train), axis=1)\n",
    "        features_test = np.concatenate((features_test, f_test), axis=1)   \n",
    "    return (features_train, features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "def Feature_Selection(x_train_fs, x_test_fs,Y_train_fs, c, igram, b, mode='tfidf'):\n",
    "    '''The function use a logistic regression with L1 regulation to reduce the dimension-ality of the data. \n",
    "    input twitters , corresponding labels, and parameters.\n",
    "    Return matrix with regularized(reduced) features '''\n",
    "    features_train, features_test = features(x_train_fs, x_test_fs, ngram=(1,igram), b=b, mc=5, ngram_p=(1,1), b_p=1, mc_p=2, other_f=1, mode=mode)\n",
    "    select = SelectFromModel(LogisticRegression(class_weight='balanced',penalty=\"l1\",C=c))\n",
    "    X_train_fs = select.fit_transform(features_train,Y_train_fs)   \n",
    "    X_test_fs = select.transform(features_test)\n",
    "    return (X_train_fs, X_test_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.tweet\n",
    "y_train = train['class'].astype(int)\n",
    "X_test = test.tweet\n",
    "y_test = test['class'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(X, y):\n",
    "    pipe = Pipeline(\n",
    "            [('select', SelectFromModel(LogisticRegression(class_weight='balanced',\n",
    "                                                      penalty=\"l1\", C=0.01))),\n",
    "            ('model', LogisticRegression(class_weight='balanced',penalty='l2'))])\n",
    "    param_grid = [{}] # Optionally add parameters here\n",
    "    grid_search = GridSearchCV(pipe, param_grid, cv=StratifiedKFold(n_splits=5, random_state=42).split(X, y), \n",
    "                               verbose=2)\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "igram=1, binary=0, mode=count\n",
      "feature selected\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=   3.5s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=   3.5s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   2.7s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   2.9s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   15.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "igram=1, binary=0, mode=count\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.50      0.43       232\n",
      "           1       0.97      0.84      0.90      3063\n",
      "           2       0.64      0.97      0.77       678\n",
      "\n",
      "    accuracy                           0.84      3973\n",
      "   macro avg       0.66      0.77      0.70      3973\n",
      "weighted avg       0.88      0.84      0.85      3973\n",
      "\n",
      "-------\n",
      "igram=1, binary=0, mode=tfidf\n",
      "feature selected\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=   2.9s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=   4.1s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   3.2s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   2.9s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   4.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   17.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "igram=1, binary=0, mode=tfidf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.51      0.48       232\n",
      "           1       0.97      0.85      0.90      3063\n",
      "           2       0.62      0.93      0.75       678\n",
      "\n",
      "    accuracy                           0.84      3973\n",
      "   macro avg       0.68      0.76      0.71      3973\n",
      "weighted avg       0.88      0.84      0.85      3973\n",
      "\n",
      "-------\n",
      "igram=1, binary=1, mode=count\n",
      "feature selected\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=   2.7s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=   4.2s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   2.3s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   2.6s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   2.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   14.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "igram=1, binary=1, mode=count\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.52      0.43       232\n",
      "           1       0.97      0.84      0.90      3063\n",
      "           2       0.64      0.97      0.77       678\n",
      "\n",
      "    accuracy                           0.84      3973\n",
      "   macro avg       0.66      0.77      0.70      3973\n",
      "weighted avg       0.88      0.84      0.85      3973\n",
      "\n",
      "-------\n",
      "igram=1, binary=1, mode=tfidf\n",
      "feature selected\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=   2.1s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=   3.4s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   2.9s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   2.5s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   2.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   13.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "igram=1, binary=1, mode=tfidf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.52      0.48       232\n",
      "           1       0.97      0.85      0.90      3063\n",
      "           2       0.63      0.94      0.75       678\n",
      "\n",
      "    accuracy                           0.84      3973\n",
      "   macro avg       0.68      0.77      0.71      3973\n",
      "weighted avg       0.88      0.84      0.85      3973\n",
      "\n",
      "-------\n",
      "igram=2, binary=0, mode=count\n",
      "feature selected\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=   2.2s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=   2.6s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   2.2s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   2.8s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   2.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   12.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "igram=2, binary=0, mode=count\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.50      0.43       232\n",
      "           1       0.97      0.84      0.90      3063\n",
      "           2       0.64      0.97      0.77       678\n",
      "\n",
      "    accuracy                           0.84      3973\n",
      "   macro avg       0.66      0.77      0.70      3973\n",
      "weighted avg       0.88      0.84      0.85      3973\n",
      "\n",
      "-------\n",
      "igram=2, binary=0, mode=tfidf\n",
      "feature selected\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=   2.3s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=   3.4s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   2.5s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   2.3s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   2.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   13.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "igram=2, binary=0, mode=tfidf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.43      0.44       232\n",
      "           1       0.96      0.86      0.91      3063\n",
      "           2       0.63      0.92      0.75       678\n",
      "\n",
      "    accuracy                           0.85      3973\n",
      "   macro avg       0.68      0.74      0.70      3973\n",
      "weighted avg       0.87      0.85      0.85      3973\n",
      "\n",
      "-------\n",
      "igram=2, binary=1, mode=count\n",
      "feature selected\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=   2.5s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=   3.9s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   1.9s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   2.3s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   2.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   13.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "igram=2, binary=1, mode=count\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.52      0.43       232\n",
      "           1       0.97      0.84      0.90      3063\n",
      "           2       0.64      0.97      0.77       678\n",
      "\n",
      "    accuracy                           0.84      3973\n",
      "   macro avg       0.66      0.77      0.70      3973\n",
      "weighted avg       0.88      0.84      0.85      3973\n",
      "\n",
      "-------\n",
      "igram=2, binary=1, mode=tfidf\n",
      "feature selected\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=   2.1s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=   3.1s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   2.5s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   3.1s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   2.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   13.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "igram=2, binary=1, mode=tfidf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.44      0.45       232\n",
      "           1       0.96      0.86      0.91      3063\n",
      "           2       0.62      0.92      0.74       678\n",
      "\n",
      "    accuracy                           0.85      3973\n",
      "   macro avg       0.68      0.74      0.70      3973\n",
      "weighted avg       0.87      0.85      0.85      3973\n",
      "\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "for n_gram in [1,2]:\n",
    "    for binary in [0,1]:\n",
    "        for mode in ['count', 'tfidf']:\n",
    "            print('igram={0}, binary={1}, mode={2}'.format(n_gram,binary,mode))\n",
    "            X_train_s, X_test_s = Feature_Selection(X_train, X_test, y_train, c=0.01, igram=n_gram, b=binary, mode=mode)\n",
    "            print(\"feature selected\")\n",
    "            grid_search = build_model(X_train_s, y_train)\n",
    "            model = grid_search.fit(X_train_s, y_train)\n",
    "            y_preds = model.predict(X_test_s)\n",
    "            report = classification_report(y_test, y_preds)\n",
    "            print('igram={0}, binary={1}, mode={2}'.format(n_gram,binary,mode))\n",
    "            print(report + '\\n' + '-------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Best \n",
    "> igram=2, binary=1, mode=tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "df = pd.read_csv(\"data/hate_speech_data_train.csv\")\n",
    "X_train = df.tweet\n",
    "y_train = df['class'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission\n",
    "df_submission = pd.read_csv(\"data/hate_speech_data_test_wo_labels.csv\", sep=\"\\t\")\n",
    "X_submission = df_submission.tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature selected\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=   5.8s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    5.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=   7.4s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   6.2s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   8.9s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   6.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   34.9s finished\n"
     ]
    }
   ],
   "source": [
    "X_train_s, X_submission_s = Feature_Selection(X_train, X_submission, y_train, c=0.01, igram=2, b=1, mode=\"tfidf\")\n",
    "print(\"feature selected\")\n",
    "grid_search = build_model(X_train_s, y_train)\n",
    "model = grid_search.fit(X_train_s, y_train)\n",
    "y_preds = model.predict(X_submission_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission['class'] = y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @WayneL_Jr: 80% RT @NotMorris_: 60% of thes...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>real nigga shit she wanna be a righteous young...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@That1guyJeff thank you. I hate you. Eat poiso...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&amp;#128514;&amp;#128514;&amp;#128514; RT @A2daO: When a ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I swear a nicca sleep but im up .. Tired as fuk!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  class\n",
       "0  RT @WayneL_Jr: 80% RT @NotMorris_: 60% of thes...      1\n",
       "1  real nigga shit she wanna be a righteous young...      1\n",
       "2  @That1guyJeff thank you. I hate you. Eat poiso...      1\n",
       "3  &#128514;&#128514;&#128514; RT @A2daO: When a ...      2\n",
       "4   I swear a nicca sleep but im up .. Tired as fuk!      1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submission.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.to_csv(\"submission/4_submission.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
