{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "import re\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "from textstat.textstat import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "%matplotlib inline\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @__Junebugg: @VoiceOfDStreetz hell yea save...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>How the hell was David Murphy's hit not a home...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @FeeelGreatness: You don't know where your ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Thats some hoe shit doe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>I just want vanilla Oreos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19821</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @davegetnmoney: I beat the pussy up up up u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19822</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @RT_America: Russell Brand mocks Bill O'Rei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19823</th>\n",
       "      <td>1</td>\n",
       "      <td>@_B_R_Y_C_E_ what happened to going fishing bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19824</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @BriannDominguez: Gasoline - daddy Yankee &amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19825</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @LowkeyStoner_: Gay girls really do get mor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19826 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       class                                              tweet\n",
       "0          1  RT @__Junebugg: @VoiceOfDStreetz hell yea save...\n",
       "1          2  How the hell was David Murphy's hit not a home...\n",
       "2          1  RT @FeeelGreatness: You don't know where your ...\n",
       "3          1                            Thats some hoe shit doe\n",
       "4          2                          I just want vanilla Oreos\n",
       "...      ...                                                ...\n",
       "19821      1  RT @davegetnmoney: I beat the pussy up up up u...\n",
       "19822      2  RT @RT_America: Russell Brand mocks Bill O'Rei...\n",
       "19823      1  @_B_R_Y_C_E_ what happened to going fishing bi...\n",
       "19824      2  RT @BriannDominguez: Gasoline - daddy Yankee &...\n",
       "19825      1  RT @LowkeyStoner_: Gay girls really do get mor...\n",
       "\n",
       "[19826 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/hate_speech_data_train.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['class', 'tweet'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_pickle_file='pickle/mask_pickle'\n",
    "if os.path.exists(mask_pickle_file):\n",
    "    file = open(mask_pickle_file,'rb')\n",
    "    msk = pickle.load(file, encoding='latin1')\n",
    "else:\n",
    "    msk = np.random.rand(len(df)) < 0.8\n",
    "    outfile = open(mask_pickle_file,'wb')\n",
    "    pickle.dump(msk,outfile)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15853\n",
      "3973\n"
     ]
    }
   ],
   "source": [
    "train = df[msk]\n",
    "test = df[~msk]\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12315\n",
       "2     2618\n",
       "0      920\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3063\n",
       "2     678\n",
       "0     232\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns key:\n",
    "count = number of CrowdFlower users who coded each tweet (min is 3, sometimes more users coded a tweet when judgments were determined to be unreliable by CF).\n",
    "\n",
    "\n",
    "hate_speech = number of CF users who judged the tweet to be hate speech.\n",
    "\n",
    "\n",
    "offensive_language = number of CF users who judged the tweet to be offensive.\n",
    "\n",
    "\n",
    "neither = number of CF users who judged the tweet to be neither offensive nor non-offensive.\n",
    "\n",
    "\n",
    "class = class label for majority of CF users.\n",
    "\n",
    "    0 - hate speech\n",
    "    1 - offensive  language\n",
    "    2 - neither\n",
    "\n",
    "tweet = raw tweet text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x126bceef0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['class'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This histogram shows the imbalanced nature of the task - most tweets containing \"hate\" words as defined by Hatebase were \n",
    "only considered to be offensive by the CF coders. More tweets were considered to be neither hate speech nor offensive language than were considered hate speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=df.tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yakupgorur/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/yakupgorur/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
    "stopwords.extend(other_exclusions)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def preprocess(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    return parsed_text\n",
    "\n",
    "def tokenize(tweet):\n",
    "    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
    "    and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z]*\", tweet.lower())).strip()\n",
    "    tokens = [stemmer.stem(t) for t in tweet.split()]\n",
    "    return tokens\n",
    "\n",
    "def basic_tokenize(tweet):\n",
    "    \"\"\"Same as tokenize but without the stemming\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z.,!?]*\", tweet.lower())).strip()\n",
    "    return tweet.split()\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=tokenize,\n",
    "    preprocessor=preprocess,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=stopwords,\n",
    "    use_idf=True,\n",
    "    smooth_idf=False,\n",
    "    norm=None,\n",
    "    decode_error='replace',\n",
    "    max_features=10000,\n",
    "    min_df=5,\n",
    "    max_df=0.75\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yakupgorur/opt/anaconda3/envs/alterna/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'e', 'f', 'g', 'h', 'j', 'l', 'n', 'p', 'r', 'u', 'v', 'w'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "#Construct tfidf matrix and get relevant scores\n",
    "tfidf = vectorizer.fit_transform(tweets).toarray()\n",
    "vocab = {v:i for i, v in enumerate(vectorizer.get_feature_names())}\n",
    "idf_vals = vectorizer.idf_\n",
    "idf_dict = {i:idf_vals[i] for i in vocab.values()} #keys are indices; values are IDF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get POS tags for tweets and save as a string\n",
    "tweet_tags = []\n",
    "for t in tweets:\n",
    "    tokens = basic_tokenize(preprocess(t))\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    tag_list = [x[1] for x in tags]\n",
    "    tag_str = \" \".join(tag_list)\n",
    "    tweet_tags.append(tag_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use the TFIDF vectorizer to get a token matrix for the POS tags\n",
    "pos_vectorizer = TfidfVectorizer(\n",
    "    tokenizer=None,\n",
    "    lowercase=False,\n",
    "    preprocessor=None,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=None,\n",
    "    use_idf=False,\n",
    "    smooth_idf=False,\n",
    "    norm=None,\n",
    "    decode_error='replace',\n",
    "    max_features=5000,\n",
    "    min_df=5,\n",
    "    max_df=0.75,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct POS TF matrix and get vocab dict\n",
    "pos = pos_vectorizer.fit_transform(pd.Series(tweet_tags)).toarray()\n",
    "pos_vocab = {v:i for i, v in enumerate(pos_vectorizer.get_feature_names())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now get other features\n",
    "sentiment_analyzer = VS()\n",
    "\n",
    "def count_twitter_objs(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "    4) hashtags with HASHTAGHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned.\n",
    "    \n",
    "    Returns counts of urls, mentions, and hashtags.\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)\n",
    "    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)\n",
    "    return(parsed_text.count('URLHERE'),parsed_text.count('MENTIONHERE'),parsed_text.count('HASHTAGHERE'))\n",
    "\n",
    "def other_features(tweet):\n",
    "    \"\"\"This function takes a string and returns a list of features.\n",
    "    These include Sentiment scores, Text and Readability scores,\n",
    "    as well as Twitter specific features\"\"\"\n",
    "    sentiment = sentiment_analyzer.polarity_scores(tweet)\n",
    "    \n",
    "    words = preprocess(tweet) #Get text only\n",
    "    \n",
    "    syllables = textstat.syllable_count(words)\n",
    "    num_chars = sum(len(w) for w in words)\n",
    "    num_chars_total = len(tweet)\n",
    "    num_terms = len(tweet.split())\n",
    "    num_words = len(words.split())\n",
    "    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_terms = len(set(words.split()))\n",
    "    \n",
    "    ###Modified FK grade, where avg words per sentence is just num words/1\n",
    "    FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59,1)\n",
    "    ##Modified FRE score, where sentence fixed to 1\n",
    "    FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)),2)\n",
    "    \n",
    "    twitter_objs = count_twitter_objs(tweet)\n",
    "    retweet = 0\n",
    "    if \"rt\" in words:\n",
    "        retweet = 1\n",
    "    features = [FKRA, FRE,syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,\n",
    "                num_unique_terms, sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound'],\n",
    "                twitter_objs[2], twitter_objs[1],\n",
    "                twitter_objs[0], retweet]\n",
    "    #features = pandas.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "def get_feature_array(tweets):\n",
    "    feats=[]\n",
    "    for t in tweets:\n",
    "        feats.append(other_features(t))\n",
    "    return np.array(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_features_names = [\"FKRA\", \"FRE\",\"num_syllables\", \"avg_syl_per_word\", \"num_chars\", \"num_chars_total\", \\\n",
    "                        \"num_terms\", \"num_words\", \"num_unique_words\", \"vader neg\",\"vader pos\",\"vader neu\", \\\n",
    "                        \"vader compound\", \"num_hashtags\", \"num_mentions\", \"num_urls\", \"is_retweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = get_feature_array(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now join them all up\n",
    "M = np.concatenate([tfidf,pos,feats],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15853, 3639)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally get a list of variable names\n",
    "variables = ['']*len(vocab)\n",
    "for k,v in vocab.items():\n",
    "    variables[v] = k\n",
    "\n",
    "pos_variables = ['']*len(pos_vocab)\n",
    "for k,v in pos_vocab.items():\n",
    "    pos_variables[v] = k\n",
    "\n",
    "feature_names = variables+pos_variables+other_features_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the model\n",
    "\n",
    "The best model was selected using a GridSearch with 5-fold CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_models(X, y, X_test, y_test): \n",
    "\n",
    "    clfs = []\n",
    "    clfs.append(LogisticRegression())\n",
    "    clfs.append(SVC())\n",
    "    clfs.append(KNeighborsClassifier(n_neighbors=3))\n",
    "    clfs.append(DecisionTreeClassifier())\n",
    "    clfs.append(RandomForestClassifier())\n",
    "    clfs.append(GradientBoostingClassifier())\n",
    "    score_dict={}\n",
    "    for model in clfs:\n",
    "        model_name = type(model).__name__\n",
    "        print(\"---\" + type(model).__name__ + \" is running\" + \"---\")\n",
    "        param_grid = [{}] # Optionally add parameters here\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=StratifiedKFold(n_splits=5, random_state=42).split(X, y), verbose=2)\n",
    "        model = grid_search.fit(X, y)\n",
    "        y_preds = model.predict(X_test)\n",
    "        report = classification_report( y_test, y_preds )\n",
    "        score_dict[model_name] = report\n",
    "    \n",
    "    return score_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(M)\n",
    "y = df['class'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.tweet\n",
    "tweet_tags = []\n",
    "for t in X_test:\n",
    "    tokens = basic_tokenize(preprocess(t))\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    tag_list = [x[1] for x in tags]\n",
    "    tag_str = \" \".join(tag_list)\n",
    "    tweet_tags.append(tag_str)\n",
    "tfidf = vectorizer.transform(X_test).toarray()\n",
    "pos = pos_vectorizer.transform(pd.Series(tweet_tags)).toarray()\n",
    "feats = get_feature_array(X_test)\n",
    "\n",
    "M = np.concatenate([tfidf,pos,feats],axis=1)\n",
    "X_test_vectorized = pd.DataFrame(M)\n",
    "y_test = test['class'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---LogisticRegression is running---\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=  11.5s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   11.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=   9.3s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   7.7s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  12.9s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  10.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   51.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---SVC is running---\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total= 9.7min\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  9.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total= 8.5min\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total= 8.3min\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total= 8.5min\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total= 8.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 43.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---KNeighborsClassifier is running---\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total= 1.2min\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total= 1.2min\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total= 1.2min\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total= 1.2min\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---DecisionTreeClassifier is running---\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=  35.3s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   35.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=  31.3s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  29.9s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  28.9s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  29.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RandomForestClassifier is running---\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=   2.8s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=   2.7s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   2.7s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   2.9s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   2.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   14.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GradientBoostingClassifier is running---\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=12.3min\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 12.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=12.3min\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=12.3min\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=12.4min\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=12.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 61.7min finished\n"
     ]
    }
   ],
   "source": [
    "scores = forecast_models(X, y, X_test_vectorized, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----LogisticRegression report:----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.22      0.23       232\n",
      "           1       0.88      0.90      0.89      3063\n",
      "           2       0.67      0.62      0.64       678\n",
      "\n",
      "    accuracy                           0.81      3973\n",
      "   macro avg       0.59      0.58      0.59      3973\n",
      "weighted avg       0.81      0.81      0.81      3973\n",
      "\n",
      "----SVC report:----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       232\n",
      "           1       0.82      0.98      0.90      3063\n",
      "           2       0.77      0.37      0.50       678\n",
      "\n",
      "    accuracy                           0.82      3973\n",
      "   macro avg       0.53      0.45      0.46      3973\n",
      "weighted avg       0.77      0.82      0.78      3973\n",
      "\n",
      "----KNeighborsClassifier report:----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.07      0.10       232\n",
      "           1       0.79      0.97      0.87      3063\n",
      "           2       0.55      0.10      0.17       678\n",
      "\n",
      "    accuracy                           0.77      3973\n",
      "   macro avg       0.51      0.38      0.38      3973\n",
      "weighted avg       0.71      0.77      0.70      3973\n",
      "\n",
      "----DecisionTreeClassifier report:----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.25      0.24       232\n",
      "           1       0.87      0.87      0.87      3063\n",
      "           2       0.57      0.55      0.56       678\n",
      "\n",
      "    accuracy                           0.78      3973\n",
      "   macro avg       0.56      0.56      0.56      3973\n",
      "weighted avg       0.78      0.78      0.78      3973\n",
      "\n",
      "----RandomForestClassifier report:----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.04      0.07       232\n",
      "           1       0.82      0.97      0.89      3063\n",
      "           2       0.73      0.35      0.47       678\n",
      "\n",
      "    accuracy                           0.81      3973\n",
      "   macro avg       0.65      0.45      0.48      3973\n",
      "weighted avg       0.78      0.81      0.77      3973\n",
      "\n",
      "----GradientBoostingClassifier report:----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.14      0.22       232\n",
      "           1       0.86      0.95      0.90      3063\n",
      "           2       0.73      0.57      0.64       678\n",
      "\n",
      "    accuracy                           0.84      3973\n",
      "   macro avg       0.70      0.55      0.59      3973\n",
      "weighted avg       0.82      0.84      0.82      3973\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in scores.keys():\n",
    "    print(\"----\" + key + \" report:\" + \"----\")\n",
    "    print(scores[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
